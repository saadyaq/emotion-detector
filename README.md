# ðŸŽ™ï¸ Emotion Detector

This project aims to build a **Speech Emotion Recognition** (SER) system that can detect human emotions from vocal signals using both **classical ML** (Random Forest, MLP) and **deep learning** (CNN on spectrograms). We built an end-to-end pipeline, from raw audio to a real-time Streamlit application.

---

## ðŸ“ Project Structure

```
emotion_detector/
â”œâ”€â”€ data/                      # Raw and processed data
â”‚   â”œâ”€â”€ raw/                  # RAVDESS dataset
â”‚   â”œâ”€â”€ metadata.csv          # Paths + emotion labels
â”‚   â”œâ”€â”€ X.npy / y.npy         # Features and targets (classical)
â”‚   â””â”€â”€ X_spectro.npy         # Image-based features
â”œâ”€â”€ models/                   # Saved models (e.g. Random Forest)
â”‚   â””â”€â”€ rf.joblib
â”œâ”€â”€ src/                      # Python modules
â”‚   â”œâ”€â”€ feature_extraction.py
â”‚   â”œâ”€â”€ preprocess.py
â”‚   â”œâ”€â”€ audio_to_image.py
â”‚   â”œâ”€â”€ train.py
â”‚   â””â”€â”€ predict.py
â”œâ”€â”€ notebooks/                # Jupyter notebooks
â”‚   â”œâ”€â”€ exploration.ipynb
â”‚   â”œâ”€â”€ build_dataset.ipynb
â”‚   â””â”€â”€ indexation.ipynb
â”œâ”€â”€ app.py                    # Streamlit app for real-time prediction
â”œâ”€â”€ requirements.txt          # Python dependencies
â””â”€â”€ README.md                 # Project description
```

---

## ðŸš€ To Run

Install dependencies:

```bash
pip install -r requirements.txt
```

Then launch the Streamlit app:

```bash
streamlit run app.py
```

---

## ðŸ“Š What We've Built

### ðŸ§© 1. Dataset Preparation

- Used the **RAVDESS dataset** (Ryerson Audio-Visual Database of Emotional Speech and Song).
- Created a structured `metadata.csv` file with paths and corresponding emotion labels.

### ðŸŽµ 2. Audio Feature Extraction

From `.wav` files, we extracted features using `librosa`:
- **MFCCs**
- **Chroma STFT**
- **Spectral Centroid**
- **Spectral Bandwidth**
- **Spectral Contrast**
- **Tempo**

This produced a numerical dataset saved as `X.npy` / `y.npy`.

### ðŸ§  3. Machine Learning Models

We trained various classifiers:
- **Logistic Regression**
- **MLP Classifier**
- **Random Forest (best result â‰ˆ 44%)**

We standardized and optionally applied PCA to reduce dimensions.

### ðŸ–¼ï¸ 4. CNN with Spectrograms

We created mel-spectrogram images for each `.wav` using `librosa`:
- Normalized and padded to fixed size (128x143x1)
- Saved as numpy array `X_spectro.npy`

Then trained a CNN with TensorFlow/Keras:
- 3 convolutional layers
- BatchNorm, Dropout
- Accuracy ~ 28%

### ðŸŒ 5. Streamlit App

- Record from microphone
- Extract features
- Load trained model
- Predict & display emotion

> Note: works only on native OS (not WSL) for microphone.

---

## ðŸ“Œ Improvements Planned

- Improve CNN architecture (attention, deeper layers, transfer learning)
- Use data augmentation
- Try other audio features (e.g., pitch, energy)
- Deploy the app online (e.g., Streamlit Cloud or Hugging Face Spaces)

---

## ðŸ“š References

- RAVDESS dataset: https://zenodo.org/record/1188976
- Librosa documentation: https://librosa.org
- TensorFlow, Scikit-learn, Streamlit

---

## âœ¨ Author

**Saad Yaqine**

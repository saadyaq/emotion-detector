# ğŸ¤ Emotion Detector (Multilingual)

This project detects emotions in multilingual speech using audio signal processing, `librosa`, and machine learning.

## ğŸ” Features
- Raw `.wav` audio exploration
- Waveform and Mel spectrogram visualization
- MFCC and feature extraction with `librosa`
- Ready for model training and Streamlit app

## ğŸ“ Structure
emotion_detector/ â”‚ â”œâ”€â”€ data/ # Raw and processed data â”‚ â”œâ”€â”€ raw/ # RAVDESS dataset â”‚ â”œâ”€â”€ metadata.csv # Paths + emotion labels â”‚ â”œâ”€â”€ X.npy / y.npy # Features and targets (classical) â”‚ â””â”€â”€ X_spectro.npy # Image-based features â”‚ â”œâ”€â”€ models/ # Saved models (e.g. Random Forest) â”‚ â””â”€â”€ rf.joblib â”‚ â”œâ”€â”€ src/ # Python modules â”‚ â”œâ”€â”€ feature_extraction.py â”‚ â”œâ”€â”€ preprocess.py â”‚ â”œâ”€â”€ audio_to_image.py â”‚ â””â”€â”€ train.py â”‚ â”œâ”€â”€ notebooks/ # Jupyter notebooks â”‚ â”œâ”€â”€ exploration.ipynb â”‚ â”œâ”€â”€ build_dataset.ipynb â”‚ â””â”€â”€ indexation.ipynb â”‚ â”œâ”€â”€ app.py # Streamlit app for real-time prediction â”œâ”€â”€ requirements.txt # Python dependencies â””â”€â”€ README.md # Project description
## ğŸš€ To Run
```bash
pip install -r requirements.txt## ğŸ”§ What We've Built

### ğŸ—‚ï¸ 1. Dataset Preparation
- Used the **RAVDESS dataset** (Ryerson Audio-Visual Database of Emotional Speech and Song).
- Created a structured `metadata.csv` with file paths and corresponding emotion labels.

### ğŸ§ 2. Audio Feature Extraction
From each `.wav` file, we extracted features using `librosa`:
- **MFCCs**
- **Chroma STFT**
- **Spectral Centroid**
- **Spectral Bandwidth**
- **Spectral Contrast**
- **Tempo**

These were used as inputs to classical machine learning models.

### ğŸ§  3. Model Training
We trained and evaluated multiple models:
- âœ… **Random Forest** (best performance: ~44% accuracy)
- MLP (Multi-layer Perceptron)
- Logistic Regression
- CNN with spectrograms (TensorFlow)
- PyTorch (experimental)

The **Random Forest** achieved the best balance of simplicity and performance.

### ğŸ“· 4. Spectrogram Approach (CNN)
- Converted audio signals to **Mel spectrograms**.
- Normalized and padded them to fixed shapes.
- Built a CNN in TensorFlow/Keras.
- While results were below the RF model, this approach has strong potential.

### ğŸŒ 5. Real-Time Web App (Streamlit)
Built a live Streamlit interface:
- ğŸ™ï¸ Records audio using the microphone.
- ğŸ›ï¸ Extracts features in real-time.
- ğŸ¤– Uses the pre-trained Random Forest to predict emotion.
- ğŸ“ˆ Displays prediction on the interface.

---

## ğŸ§ª How to Run

```bash
# Create & activate a virtual environment
python -m venv venv
source venv/bin/activate  # or .\venv\Scripts\activate on Windows

# Install dependencies
pip install -r requirements.txt

# Run the app
streamlit run app.py

---
## ğŸ“ˆ Known Challenges

- Dataset is small (~1400 files)
- Emotion categories are unbalanced (e.g., fewer neutral samples)
- Emotion in speech is subtle and subjective
- CNNs require larger data or transfer learning

---

## ğŸš€ Future Work

- Improve CNN with deeper architecture & data augmentation
- Use **pre-trained models** (YAMNet, wav2vec, etc.)
- Try **RNNs / LSTMs** for temporal modeling
- Multi-modal fusion (e.g., facial + vocal emotion)
- Deploy Streamlit on the cloud (e.g., Streamlit Cloud / Heroku / EC2)
- Add **language detection** and **multi-language emotion support**

---

## ğŸ¤ Credits

This project was developed by **Saad Yaqine** as part of a personal initiative to combine signal processing, machine learning, and real-time applications.

ğŸ“¬ saadyaqine91@gmail.com

---

## ğŸ”— References

- [RAVDESS dataset](https://zenodo.org/record/1188976)
- [Librosa documentation](https://librosa.org/doc/latest/index.html)
- [Streamlit](https://streamlit.io)
- Scikit-learn, TensorFlow, PyTorch, and more!
---


